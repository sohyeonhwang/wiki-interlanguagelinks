{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re, os, csv\n",
    "import wikifunctions_sohyeon as wfs\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_10 = ['en','fr','de','ja','es','ru','zh','it','pt','fa']\n",
    "langs_5 = ['en','fr','de','ja','es']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all pages in the Wikipedia Project namespace per language edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of all pages in the wikipedia project namespace per language edition\n",
    "rule_dfs_dict = {}\n",
    "rule_dfs_list = []\n",
    "\n",
    "for l in langs_10:\n",
    "    _temp = wfs.get_all_pages_in_namespace(l)\n",
    "    rule_dfs_dict[l] = _temp\n",
    "    rule_dfs_list.append(_temp)\n",
    "    print('{} has {} pages with language links in the Wikipedia Project namespace'.format(l, len(_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export each list as df -> tsv so that we don't have to get them all over again\n",
    "output_directory = './nm4/nm4_all_list'\n",
    "\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "for df in rule_dfs_list:\n",
    "    _lang = df.lang.values.tolist()[0]\n",
    "    path = './{}/{}.tsv'.format(output_directory,_lang)\n",
    "    df.to_csv(path,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the interlanguage links for the pages in the Wikipedia project namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load those dfs and get all the ILLs for the pages\n",
    "rule_dfs_dict = {}\n",
    "rule_dfs_list = []\n",
    "\n",
    "for l in langs_10:\n",
    "    path = './nm4/nm4_all_list/{}.tsv'.format(l)\n",
    "    _df = pd.read_csv(path, sep='\\t',header=0)\n",
    "    rule_dfs_dict[l] = _df\n",
    "    rule_dfs_list.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './nm4/nm4_all_ills'\n",
    "\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block below takes like 100+ minutes to run. Strap in and work on something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interlanguage_links = {}\n",
    "\n",
    "# run through each language edition\n",
    "for lang in langs_10:\n",
    "    print(lang)\n",
    "    start = time.time()\n",
    "    interlanguage_links[lang] = {}\n",
    "    # get the list of pages\n",
    "    _df = rule_dfs_dict[lang]\n",
    "    rule_list = _df['title'].values.tolist()\n",
    "\n",
    "    # for each page\n",
    "    for rule in rule_list:\n",
    "        ill_dict = wfs.get_interlanguage_links(rule, endpoint=lang, redirects=1)\n",
    "        interlanguage_links[lang][rule] = {}\n",
    "        interlanguage_links[lang][rule]['count'] = len(list(ill_dict.keys()))\n",
    "        interlanguage_links[lang][rule]['langs'] = list(ill_dict.keys())\n",
    "        interlanguage_links[lang][rule]['links'] = ill_dict\n",
    "\n",
    "    # export the interlanguage_links[lang]\n",
    "    with open(\"./{}/ills_{}.json\".format(output_directory,lang), \"w\") as outfile:\n",
    "        json.dump(interlanguage_links[lang], outfile)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"{}: {}\".format(lang, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserving the last output of running the above code. I actually did English first, and then the rest of the language editions. English took about 45-50 minutes to do.\n",
    "\n",
    "```\n",
    "fr\n",
    "    Wikipédia:Politique de confidentialité\n",
    "    {'interwiki': [{'title': 'meta:Privacy policy/fr', 'iw': 'meta'}], 'redirects': [{'from': 'Wikipédia:Politique de confidentialité', 'to': 'meta:Privacy policy/fr', 'tointerwiki': 'meta'}]}\n",
    "fr: 1433.1813595294952\n",
    "de: 405.3607313632965\n",
    "ja: 209.69620060920715\n",
    "es: 216.3900294303894\n",
    "ru: 274.83118629455566\n",
    "zh: 687.3477938175201\n",
    "it: 105.42117857933044\n",
    "pt: 516.9702491760254\n",
    "fa: 493.1874372959137\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f = open(\"./{}/ills_en.json\".format(output_directory))\n",
    "en_data = json.load(f)\n",
    "interlanguage_links['en'] = en_data\n",
    "print(interlanguage_links.keys())\n",
    "with open(\"./{}/ills_all.json\".format(output_directory), \"w\") as outfile:\n",
    "    json.dump(interlanguage_links, outfile)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter through the pages in the Wikipedia Project namespace by the number of interlanguage links they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up interlanguage_links\n",
    "f = open(\"./{}/ills_all.json\".format(output_directory))\n",
    "interlanguage_links = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interlanguage_links.keys(), len(interlanguage_links.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten interlanguage_links and make into a df\n",
    "_data = []\n",
    "\n",
    "for lang in interlanguage_links:\n",
    "    for rule in interlanguage_links[lang]:\n",
    "        _count = interlanguage_links[lang][rule]['count']\n",
    "        _langs = interlanguage_links[lang][rule]['langs']\n",
    "        _links = interlanguage_links[lang][rule]['links']\n",
    "\n",
    "        _data.append([lang,rule,_count,_langs,_links])\n",
    "\n",
    "df = pd.DataFrame(_data,columns=['lang','title','ill_count','ill_langs','ill_links'])\n",
    "len(df[df.ill_count > 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter it by number of ILLs and whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.loc[df.ill_langs.map(set(langs_10).issubset)]\n",
    "filtered_df = filtered_df.copy()\n",
    "print(len(filtered_df))\n",
    "filtered_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in langs_10:\n",
    "    print(lang,len(filtered_df[filtered_df.lang==lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    filtered_df[filtered_df.ill_langs.map(set(['en']).issubset)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_title(row):\n",
    "    links = row.ill_links\n",
    "    return links['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['en_title'] = filtered_df.apply(get_en_title,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfiltered rules, a set of project namespace pages that are clearly in the Wikipedia: space and have an ILL to all top ten language editions\n",
    "unfiltered_rules = filtered_df.drop_duplicates(subset=['en_title'], keep='last').loc[filtered_df.en_title.str.contains('Wikipedia:')==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling these pages \"rules\" (as I suggest in variable naming) per say is a bit premature, because it does still include other non-rule but still meta project content probably.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(unfiltered_rules.lang.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_rules.sort_values(by='ill_count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 202104_rules_withillcolumns is my curated set of rules, re-structured a bit for convenience\n",
    "# i don't quite remember how it is that I generated this file, but I have my suspicions and can probably figure it out if I dig through old code again.\n",
    "\n",
    "curated_rules_ills =  pd.read_csv(\"202104_rules_withillcolumns\",sep='\\t',header=0)\n",
    "have_4_ills = curated_rules_ills.loc[curated_rules_ills.num_links ==5]\n",
    "widely_shared_rules = have_4_ills.en.values.tolist()\n",
    "len(widely_shared_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "219346b13714f768f9488e9b37d2a769c34fb63960c6dbb9230676da542bbf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
